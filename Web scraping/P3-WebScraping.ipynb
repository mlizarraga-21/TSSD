{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica 3 - Web Scraping y Data Wrangling\n",
    "\n",
    "### Mario Alberto Lizarraga Reyes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Práctica 1\n",
    "\n",
    "Usando requests recupere la página https://www.bloghemia.com/2019/05/noam-chomsky-michel-foucault-debate.html luego, usando beautifulsoup y pandas realizar los siguientes pasos con ella:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install requests beautifulsoup4 pandas openpyxl xlwt\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import json\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class='ltr' dir='ltr' lang='es' xmlns='http://www.w3.org/1999/xhtml' xmlns:b='http://www.google.com/2005/gml/b' xmlns:data='http://www.google.com/2005/gml/data' xmlns:expr='http://www.google.com/2005/gml/expr'>\n",
      "<head>\n",
      "<link href='https://www.bloghemia.com/2019/05/noam-chomsky-michel-foucault-debate.html' rel='canonical'/>\n",
      "<script async='async' data-ad-client='ca-pub-3126310743890539' src='https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js'></script>\n",
      "<title>Justicia vs Poder |  Transcripción completa al español</title>\n",
      "<!-- Descripción optimizada -->\n",
      "<!-- Descripción dinámica para entradas y otras páginas -->\n",
      "<meta content='Transcripción del Debate entre Noam Chomsky y Michel Foucault, celebrado en la Universidad de Amsterdam ' name='description'/>\n",
      "<meta content='Bloghemia' name='author'/>\n",
      "<meta content='https://www.bloghemia.com/' name='identifier-url'/>\n",
      "<meta content='index, follow' name='robots'/>\n",
      "<meta content='Bloghemia' name='Copyright'/>\n",
      "<meta co\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# URL del debate\n",
    "url = \"https://www.bloghemia.com/2019/05/noam-chomsky-michel-foucault-debate.html\"\n",
    "\n",
    "# Obtener el contenido de la página\n",
    "response = requests.get(url)\n",
    "response.encoding = 'utf-8'  # Asegurar la codificación correcta\n",
    "html_content = response.text\n",
    "\n",
    "# Parsear el contenido HTML con BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Mostrar los primeros 1000 caracteres para verificar que la página se haya descargado correctamente\n",
    "print(html_content[:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Extraer su encabezado y armar un dataframe agrupando sus contenidos por tipo de etiqueta (hint: groupby), excepto por lo que esté relacionado con la inline stylesheet pues esto último se escribirá a un archivo CSS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer el encabezado\n",
    "head = soup.head\n",
    "\n",
    "# Crear un diccionario para almacenar el contenido agrupado por etiqueta\n",
    "header_tags = {}\n",
    "\n",
    "for tag in head.find_all():\n",
    "    tag_name = tag.name\n",
    "    if tag_name not in header_tags:\n",
    "        header_tags[tag_name] = []\n",
    "    header_tags[tag_name].append(str(tag))\n",
    "\n",
    "# Convertir en DataFrame\n",
    "df_header = pd.DataFrame([(key, value) for key, value in header_tags.items()], columns=[\"Etiqueta\", \"Contenido\"])\n",
    "\n",
    "# Aplicar groupby para ver cuántos elementos hay por tipo de etiqueta\n",
    "df_grouped = df_header.groupby(\"Etiqueta\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminar todos los inline scripts y otras etiquetas que estén relacionados con anuncios y todo tipo de publicidad; hacer esto tanto para el encabezado como para el cuerpo del archivo HTML.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar scripts en línea\n",
    "for script in soup.find_all('script'):\n",
    "    script.decompose()\n",
    "\n",
    "# Lista de palabras clave relacionadas con anuncios\n",
    "ad_keywords = ['ad', 'advertisement', 'banner', 'promo', 'googleads', 'doubleclick', 'googlesyndication']\n",
    "\n",
    "# Eliminar etiquetas con atributos que contengan palabras clave de publicidad\n",
    "for ad_keyword in ad_keywords:\n",
    "    for element in soup.find_all(attrs={'class': lambda x: x and ad_keyword in x}):\n",
    "        element.decompose()\n",
    "    for element in soup.find_all(attrs={'id': lambda x: x and ad_keyword in x}):\n",
    "        element.decompose()\n",
    "    for element in soup.find_all('meta', attrs={'content': lambda x: x and ad_keyword in x}):\n",
    "        element.decompose()\n",
    "    for element in soup.find_all('noscript'):\n",
    "        if any(ad_keyword in str(element) for ad_keyword in ad_keywords):\n",
    "            element.decompose()\n",
    "    for element in soup.find_all('link', attrs={'href': lambda x: x and ad_keyword in x}):\n",
    "        element.decompose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este paso en específico tuve que dedicarle más tiempo ya que me seguían quedando multiples objetos de publicidad, por lo que tuve que hacer una busqueda más exhaustiva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Del dataframe del paso 1 obtenga todos los metadatos y escríbalos a un archivo Excel llamado metadata.xls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadatos guardados en metadata.xlsx\n",
      "<bound method NDFrame.head of           Contenido\n",
      "Etiqueta           \n",
      "link              1\n",
      "meta              1\n",
      "noscript          1\n",
      "script            1\n",
      "style             1\n",
      "title             1>\n"
     ]
    }
   ],
   "source": [
    "# Guardar los metadatos en un archivo Excel\n",
    "metadata_path = \"metadata.xlsx\"\n",
    "df_grouped.to_excel(metadata_path, index=False)\n",
    "\n",
    "print(f\"Metadatos guardados en {metadata_path}\")\n",
    "print(df_grouped.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Con el contenido del cuerpo se van a generar dos archivos:\n",
    "\n",
    "#### A). Un archivo HTML “mínimo” donde solamente estarán (en orden) las intervenciones de las 3 personas que intervinieron en el debate. Para el encabezado de este HTML puede usar el título del documento fuente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar otros elementos irrelevantes (iframes, estilos en línea, etc.)\n",
    "for element in soup([\"iframe\", \"ins\", \"noscript\", \"style\"]):\n",
    "    element.decompose()\n",
    "\n",
    "# Extraer el contenido limpio\n",
    "clean_html = str(soup)\n",
    "\n",
    "# Guardar la versión limpia en un archivo\n",
    "html_clean_path = \"clean_page.html\"\n",
    "with open(html_clean_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(clean_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este archivo, realmente sigo teniendo muchos elementos irrelevantes, sin embargo, encontré muchos problemas al querer limpiar más, llegando incluso a borrar todo y no pude refinar la limpieza para quedarme solo con lo necesario, por lo que por el momento lo dejaré así"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B). Un archivo JSON producto de un dataframe donde se agrupen las intervenciones de cada interlocutor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer todas las etiquetas <span> con intervenciones\n",
    "span_tags = soup.find_all(\"span\")\n",
    "\n",
    "# Inicializar variables\n",
    "debate_data = []\n",
    "current_speaker = None\n",
    "\n",
    "# Analizar cada etiqueta span\n",
    "for span in span_tags:\n",
    "    text = span.get_text(strip=True)\n",
    "\n",
    "    # Detectar cambios de interlocutor\n",
    "    if text.startswith((\"ELDERS:\", \"CHOMSKY:\", \"FOUCAULT:\")):\n",
    "        current_speaker = text.split(\":\")[0]\n",
    "        text = text[len(current_speaker) + 1:].strip()  # Quitar el nombre del hablante\n",
    "\n",
    "    # Si seguimos con el mismo hablante, agregar la intervención\n",
    "    if current_speaker:\n",
    "        debate_data.append({\"Interlocutor\": current_speaker, \"Intervención\": text})\n",
    "\n",
    "# Convertir en DataFrame\n",
    "df_debate = pd.DataFrame(debate_data)\n",
    "\n",
    "# Agrupar las intervenciones por interlocutor\n",
    "df_grouped_debate = df_debate.groupby(\"Interlocutor\")[\"Intervención\"].apply(lambda x: \" \".join(x)).reset_index()\n",
    "\n",
    "# Guardar en JSON\n",
    "json_path = \"debate.json\"\n",
    "df_grouped_debate.to_json(json_path, orient=\"records\", indent=4, force_ascii=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este paso fue relativamente sencillo ya que solo tuve que extraer los elementos span y hacer la busqueda en base al inicio de los parrafos que mencionan a la persona al inicio de cada intervención."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Práctica 2\n",
    "\n",
    "#### Ahora use archivos html (uno o más) de alguna de las fuentes de datos que utilizará para su proyecto.\n",
    "#### Primero construya el parse tree de una página y recorralo usando los métodos disponibles (i.e., contents, children, descendants, parent, parents, next_sibling, and previous_sibling). Envíe a un archivo de texto (o bien con formateo html) el resultado de este recorrido.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado del recorrido guardado en parse_tree_traversal.txt\n"
     ]
    }
   ],
   "source": [
    "# Cargar el archivo HTML\n",
    "file_path = \"Requiem - Halopedia, the Halo wiki.html\"  # Ajusta la ruta según sea necesario\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "parse_tree_output = []\n",
    "\n",
    "# contents y children\n",
    "parse_tree_output.append(\"\\n### Body contents ###\")\n",
    "for child in soup.body.contents[:5]:  # Mostramos solo los primeros 5\n",
    "    parse_tree_output.append(str(child)[:200] + \"...\")\n",
    "\n",
    "parse_tree_output.append(\"\\n### Body children ###\")\n",
    "for child in soup.body.children:\n",
    "    parse_tree_output.append(str(child)[:200] + \"...\")\n",
    "    break  # Solo mostramos el primero para evitar contenido largo\n",
    "\n",
    "parse_tree_output.append(\"\\n### Body descendants ###\")\n",
    "for i, descendant in enumerate(soup.body.descendants):\n",
    "    if i > 5:  # Limitar a los primeros 5\n",
    "        break\n",
    "    parse_tree_output.append(str(descendant)[:200] + \"...\")\n",
    "\n",
    "# parent y parents de un elemento específico (el título h1)\n",
    "title_element = soup.find(\"h1\")\n",
    "parse_tree_output.append(\"\\n### Parent of title ###\")\n",
    "parse_tree_output.append(str(title_element.parent)[:200] + \"...\")\n",
    "\n",
    "# next_sibling y previous_sibling de un párrafo\n",
    "first_paragraph = soup.find(\"p\")\n",
    "parse_tree_output.append(\"\\n### Next sibling of first paragraph ###\")\n",
    "parse_tree_output.append(str(first_paragraph.next_sibling)[:200] + \"...\")\n",
    "\n",
    "parse_tree_output.append(\"\\n### Previous sibling of first paragraph ###\")\n",
    "parse_tree_output.append(str(first_paragraph.previous_sibling)[:200] + \"...\")\n",
    "\n",
    "# Guardar el resultado del recorrido en un archivo de texto\n",
    "parse_tree_path = \"parse_tree_traversal.txt\"\n",
    "with open(parse_tree_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\".join(parse_tree_output))\n",
    "\n",
    "print(\"Resultado del recorrido guardado en\", parse_tree_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este paso, se dependió principalmente de ciclos y de las funciones de soup para hacer append a los distintos objetos como contents y children (soup.body.contents), realmente el uso de esta función y de soup.find es lo unico que vale la pena mencionar del script ya que lo demás ya se había practicado anteriormente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Con el parse tree haga por lo menos un ejemplo con cada método de búsqueda disponible (i.e., find_all, find, select, and select_one). Haga al menos una búsqueda utilizando expresiones regulares.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total links found: 1014\n",
      "First link: <a class=\"mw-skin-nimbus-button positive-button\" href=\"https://www.halopedia.org/Special:CreateAccount\" rel=\"nofollow\"><span>Sign up</span></a>\n",
      "Infobox count: 1\n",
      "Page title: Requiem\n",
      "Occurrences of 'Requiem': 114\n"
     ]
    }
   ],
   "source": [
    "# find_all para encontrar todos los enlaces\n",
    "all_links = soup.find_all(\"a\")\n",
    "print(f\"Total links found: {len(all_links)}\")\n",
    "\n",
    "# find para encontrar el primer enlace\n",
    "first_link = soup.find(\"a\")\n",
    "print(f\"First link: {first_link}\")\n",
    "\n",
    "# select para buscar con CSS selector\n",
    "infobox = soup.select(\"table.infobox\")\n",
    "print(f\"Infobox count: {len(infobox)}\")\n",
    "\n",
    "# select_one para encontrar el título principal\n",
    "page_title = soup.select_one(\"h1\")\n",
    "print(f\"Page title: {page_title.text}\")\n",
    "\n",
    "# Búsqueda con expresiones regulares para encontrar textos con \"Requiem\"\n",
    "regex_search = soup.find_all(string=re.compile(\"Requiem\"))\n",
    "print(f\"Occurrences of 'Requiem': {len(regex_search)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este paso, se uso soup.find, por lo que el proceso fue muy simple y solo es cuestión de especificar el tipo de busqueda a realizar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ahora utilice los métodos disponibles para modificarlo, incluyendo agregar (i.e., append, insert, new_tag) y remover (i.e., decompose, extract).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First link removed.\n"
     ]
    }
   ],
   "source": [
    "# Agregar un nuevo párrafo\n",
    "new_paragraph = soup.new_tag(\"p\")\n",
    "new_paragraph.string = \"This is a newly inserted paragraph for testing purposes.\"\n",
    "soup.body.insert(0, new_paragraph)\n",
    "\n",
    "# Remover un elemento (por ejemplo, el primer enlace)\n",
    "if first_link:\n",
    "    first_link.decompose()\n",
    "    print(\"First link removed.\")\n",
    "\n",
    "# Insertar un nuevo encabezado\n",
    "new_heading = soup.new_tag(\"h2\")\n",
    "new_heading.string = \"New Section Added\"\n",
    "soup.body.insert(1, new_heading)\n",
    "\n",
    "# Agregar una lista desordenada\n",
    "new_list = soup.new_tag(\"ul\")\n",
    "for item_text in [\"Item 1\", \"Item 2\", \"Item 3\"]:\n",
    "    new_item = soup.new_tag(\"li\")\n",
    "    new_item.string = item_text\n",
    "    new_list.append(new_item)\n",
    "soup.body.insert(2, new_list)\n",
    "\n",
    "# Extraer y mover un párrafo al final del cuerpo\n",
    "last_paragraph = soup.find(\"p\")\n",
    "if last_paragraph:\n",
    "    extracted_paragraph = last_paragraph.extract()\n",
    "    soup.body.append(extracted_paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la modificación, se usó principalmente soup.body.insert ya que nos permite agregar diferentes tipos de objetos, para eliminar se utilizó .extract en el objetó recibido con soup.find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified HTML saved to modified_html.html\n"
     ]
    }
   ],
   "source": [
    "modified_html_path = \"modified_html.html\"\n",
    "with open(modified_html_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(soup.prettify())\n",
    "\n",
    "print(f\"Modified HTML saved to {modified_html_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pesar de que no puedo pegar imagenes aquí, se puede observar cómo algunos elementos se perdieron con las modificaciones ya que corrí el código varias veces y también se pueden ver los elementos agregados, es interesante ver los cambios generados por las modificaciones ya que se perdieron un par de elementos clave que hacen que la página cambie mucho, y se puede observar claramente la simpleza de los elementos agregados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Práctica 3\n",
    "##### Usando el archivo https://media.geeksforgeeks.org/wp-content/uploads/employees.csv cargue todo en un dataframe y a partir de ahí realice las siguientes operaciones:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  First Name  Gender Start Date Last Login Time  Salary  Bonus %  \\\n",
      "0    Douglas    Male   8/6/1993        12:42 PM   97308    6.945   \n",
      "1     Thomas    Male  3/31/1996         6:53 AM   61933    4.170   \n",
      "2      Maria  Female  4/23/1993        11:17 AM  130590   11.858   \n",
      "3      Jerry    Male   3/4/2005         1:00 PM  138705    9.340   \n",
      "4      Larry    Male  1/24/1998         4:47 PM  101004    1.389   \n",
      "\n",
      "  Senior Management             Team  \n",
      "0              True        Marketing  \n",
      "1              True              NaN  \n",
      "2             False          Finance  \n",
      "3              True          Finance  \n",
      "4              True  Client Services  \n"
     ]
    }
   ],
   "source": [
    "# Cargar el archivo CSV\n",
    "file_path = \"employees.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Despliegue solo los registros donde está faltante la información de la columna Gender (hint: isnull).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    First Name Gender  Start Date Last Login Time  Salary  Bonus %  \\\n",
      "20        Lois    NaN   4/22/1995         7:18 PM   64714    4.934   \n",
      "22      Joshua    NaN    3/8/2012         1:58 AM   90816   18.816   \n",
      "27       Scott    NaN   7/11/1991         6:58 PM  122367    5.218   \n",
      "31       Joyce    NaN   2/20/2005         2:40 PM   88657   12.752   \n",
      "41   Christine    NaN   6/28/2015         1:08 AM   66582   11.308   \n",
      "..         ...    ...         ...             ...     ...      ...   \n",
      "961    Antonio    NaN   6/18/1989         9:37 PM  103050    3.050   \n",
      "972     Victor    NaN   7/28/2006         2:49 PM   76381   11.159   \n",
      "985    Stephen    NaN   7/10/1983         8:10 PM   85668    1.909   \n",
      "989     Justin    NaN   2/10/1991         4:58 PM   38344    3.794   \n",
      "995      Henry    NaN  11/23/2014         6:09 AM  132483   16.655   \n",
      "\n",
      "    Senior Management                  Team  \n",
      "20               True                 Legal  \n",
      "22               True       Client Services  \n",
      "27              False                 Legal  \n",
      "31              False               Product  \n",
      "41               True  Business Development  \n",
      "..                ...                   ...  \n",
      "961             False                 Legal  \n",
      "972              True                 Sales  \n",
      "985             False                 Legal  \n",
      "989             False                 Legal  \n",
      "995             False          Distribution  \n",
      "\n",
      "[145 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Filtrar registros donde la columna 'Gender' es nula\n",
    "missing_gender_df = df[df[\"Gender\"].isnull()]\n",
    "\n",
    "# Mostrar los registros con valores faltantes en 'Gender'\n",
    "print(missing_gender_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ahora haga lo contrario, es decir, muestre solo los registros donde no faltan la información de la columna Gender (hint: notnull)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    First Name  Gender Start Date Last Login Time  Salary  Bonus %  \\\n",
      "0      Douglas    Male   8/6/1993        12:42 PM   97308    6.945   \n",
      "1       Thomas    Male  3/31/1996         6:53 AM   61933    4.170   \n",
      "2        Maria  Female  4/23/1993        11:17 AM  130590   11.858   \n",
      "3        Jerry    Male   3/4/2005         1:00 PM  138705    9.340   \n",
      "4        Larry    Male  1/24/1998         4:47 PM  101004    1.389   \n",
      "..         ...     ...        ...             ...     ...      ...   \n",
      "994     George    Male  6/21/2013         5:47 PM   98874    4.479   \n",
      "996    Phillip    Male  1/31/1984         6:30 AM   42392   19.675   \n",
      "997    Russell    Male  5/20/2013        12:39 PM   96914    1.421   \n",
      "998      Larry    Male  4/20/2013         4:45 PM   60500   11.985   \n",
      "999     Albert    Male  5/15/2012         6:24 PM  129949   10.169   \n",
      "\n",
      "    Senior Management                  Team  \n",
      "0                True             Marketing  \n",
      "1                True                   NaN  \n",
      "2               False               Finance  \n",
      "3                True               Finance  \n",
      "4                True       Client Services  \n",
      "..                ...                   ...  \n",
      "994              True             Marketing  \n",
      "996             False               Finance  \n",
      "997             False               Product  \n",
      "998             False  Business Development  \n",
      "999              True                 Sales  \n",
      "\n",
      "[855 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Filtrar registros donde la columna 'Gender' no es nula\n",
    "not_missing_gender_df = df[df[\"Gender\"].notnull()]\n",
    "\n",
    "# Mostrar los registros con valores no nulos en 'Gender'\n",
    "print(not_missing_gender_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### En todos los registros donde falta algún valor (en cualquier campo), remplace dicho valor (Hint: replace) con algún otro que resulte más apropiado de manejar (e.g., en Gender puede poner UNSPECIFIED, en Salary un 00.00, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  First Name  Gender Start Date Last Login Time  Salary  Bonus %  \\\n",
      "0    Douglas    Male   8/6/1993        12:42 PM   97308    6.945   \n",
      "1     Thomas    Male  3/31/1996         6:53 AM   61933    4.170   \n",
      "2      Maria  Female  4/23/1993        11:17 AM  130590   11.858   \n",
      "3      Jerry    Male   3/4/2005         1:00 PM  138705    9.340   \n",
      "4      Larry    Male  1/24/1998         4:47 PM  101004    1.389   \n",
      "\n",
      "   Senior Management             Team  \n",
      "0               True        Marketing  \n",
      "1               True          NO TEAM  \n",
      "2              False          Finance  \n",
      "3               True          Finance  \n",
      "4               True  Client Services  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lizar\\AppData\\Local\\Temp\\ipykernel_364\\2682820789.py:2: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_filled = df.fillna({\n"
     ]
    }
   ],
   "source": [
    "# Reemplazar valores nulos con valores adecuados\n",
    "df_filled = df.fillna({\n",
    "    \"Gender\": \"UNSPECIFIED\",         # Reemplazar valores nulos en 'Gender' con 'UNSPECIFIED'\n",
    "    \"Start Date\": \"UNKNOWN\",         # Reemplazar fechas faltantes con 'UNKNOWN'\n",
    "    \"Last Login Time\": \"UNKNOWN\",    # Reemplazar tiempos faltantes con 'UNKNOWN'\n",
    "    \"Salary\": 0.00,                  # Reemplazar salarios faltantes con 0.00\n",
    "    \"Bonus %\": 0.00,                 # Reemplazar valores nulos en 'Bonus %' con 0.00\n",
    "    \"Senior Management\": False,      # Asumir 'False' si falta información de senioridad\n",
    "    \"Team\": \"NO TEAM\"                # Reemplazar valores nulos en 'Team' con 'NO TEAM'\n",
    "})\n",
    "\n",
    "# Mostrar los primeros registros después de la limpieza\n",
    "print(df_filled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Muestre el número total de registros, el número donde falta al menos un valor en cualquier campo y el número de registros sin valores faltantes. Solo muestre números, no los registros mismos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "236\n",
      "764\n"
     ]
    }
   ],
   "source": [
    "# Número total de registros\n",
    "total_records = len(df)\n",
    "\n",
    "# Número de registros con al menos un valor faltante\n",
    "missing_records = df.isnull().any(axis=1).sum()\n",
    "\n",
    "# Número de registros sin valores faltantes\n",
    "complete_records = total_records - missing_records\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(total_records)\n",
    "print(missing_records)\n",
    "print(complete_records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta uiltima práctica, fue la más sencilla principalmente porque pandas ya tiene funciones built in como isnull para hacer este tipo de revisiones, lo que facilita mucho este tipo de manipulación de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A manera de conclusión, fue interesante trabajar con estas herramientas para ya una manipulación más material de lo que haremos como proyecto, en especifico la primera práctica me presentó un par de retos que fueron solucionables en su mayoría, mientras que las otras dos fueron interesantes para el uso de las distintas herramientas que nos ofrecen las librerías.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
