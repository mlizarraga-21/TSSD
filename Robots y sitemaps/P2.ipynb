{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis y Procesamiento de robots.txt y sitemaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Práctica 1\n",
    "\n",
    "Mediante su browser, lea los archivos robots.txt de los siguientes sitios, analice las directivas que allí encuentre y comente con el mayor detalle posible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.cicese.edu.mx/robots.txt\n",
    "\n",
    "La directiva de este sitio es simple ya que solo tiene disallow para dos tipos de bots, Semrush, que es un bot que se enfoca en la recopilación de datos para el análisis de marketing digital, y variaciones de este mismo bot con fines parecidos. El otro bot que bloquea es BLEXBot, el cual se enfoca en la recopilación de estructuras de URLs y interenlazado de sitios web, con esta directiva, la página indica que esos bots no deben rastrear ni indexar ninguna sección del sitio.\n",
    "\n",
    "\n",
    "User-agent: SemrushBot\n",
    "Disallow: /\n",
    "User-agent: SemrushBot-SA\n",
    "Disallow: /\n",
    "User-agent: SemrushBot-BM\n",
    "Disallow: / \n",
    "User-agent: SemrushBot-CT\n",
    "Disallow: /\n",
    "User-agent: SemrushBot-SWA\n",
    "Disallow: / \n",
    "User-agent: SemrushBot-SI\n",
    "Disallow: / \n",
    "User-agent: SemrushBot-BA\n",
    "Disallow: / \n",
    "User-agent: BLEXBot\n",
    "Disallow: /\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.uabc.mx/robots.txt\n",
    "\n",
    "El robots de este sitio tiene una directiva que aplica por igual a todos los agentes, lo cual se indica con User-agent: *, esta directiva bloquea el directorio de administración de wordpress, exceptuando un archivo (Allow: /wp-admin/admin-ajax.php). De igual manera, también bloquea /wp-content/uploads/wpforms/, que es el directorio donde se almacenan archivos de WPForms.\n",
    "Este robots, también incluye el URL para el sitemap\n",
    "\n",
    "\n",
    "\n",
    "User-agent: *\n",
    "Disallow: /wp-admin/\n",
    "Allow: /wp-admin/admin-ajax.php\n",
    "Disallow: /wp-content/uploads/wpforms/\n",
    "Sitemap: https://www.uabc.mx/wp-sitemap.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.unam.mx/robots.txt\n",
    "\n",
    "Este sitio contiene un robots mucho más detallado, el cual de inicio indica que su directiva se aplica a todos los agentes por igual y que el delay para hacer crawl es de 10 segundos, la directiva de este sitio indica no acceder a las carpetas misc, modules, profiles y themes, sin embargo, si permite acceder a los recursos css, js, a imagenes de estos, permitiendo que los motores de busqueda accedan y rendericen, pero bloqueando la información sensible que no aporta a la indexación de la página. Por ultimo, también bloquea archivos especificos como changelog, cron, e installs, y también restringe el acceso a rutas que corresponden a administración o personal para que estas no sean indexadas, por lo que la página en general restringe el acceso a áreas sensibles e irrelevantes, pero permite el rastreo de recursos como archivos css, js e imágenes.\n",
    "\n",
    "\n",
    "\n",
    "User-agent: *\n",
    "Crawl-delay: 10\n",
    "CSS, JS, Images\n",
    "Allow: /misc/*.css$\n",
    "Allow: /misc/*.css?\n",
    "Allow: /misc/*.js$\n",
    "Allow: /misc/*.js?\n",
    "Allow: /misc/*.gif\n",
    "Allow: /misc/*.jpg\n",
    "Allow: /misc/*.jpeg\n",
    "Allow: /misc/*.png\n",
    "Allow: /modules/*.css$\n",
    "Allow: /modules/*.css?\n",
    "Allow: /modules/*.js$\n",
    "Allow: /modules/*.js?\n",
    "Allow: /modules/*.gif\n",
    "Allow: /modules/*.jpg\n",
    "Allow: /modules/*.jpeg\n",
    "Allow: /modules/*.png\n",
    "Allow: /profiles/*.css$\n",
    "Allow: /profiles/*.css?\n",
    "Allow: /profiles/*.js$\n",
    "Allow: /profiles/*.js?\n",
    "Allow: /profiles/*.gif\n",
    "Allow: /profiles/*.jpg\n",
    "Allow: /profiles/*.jpeg\n",
    "Allow: /profiles/*.png\n",
    "Allow: /themes/*.css$\n",
    "Allow: /themes/*.css?\n",
    "Allow: /themes/*.js$\n",
    "Allow: /themes/*.js?\n",
    "Allow: /themes/*.gif\n",
    "Allow: /themes/*.jpg\n",
    "Allow: /themes/*.jpeg\n",
    "Allow: /themes/*.png\n",
    "Directories\n",
    "Disallow: /includes/\n",
    "Disallow: /misc/\n",
    "Disallow: /modules/\n",
    "Disallow: /profiles/\n",
    "Disallow: /scripts/\n",
    "Disallow: /themes/\n",
    "Files\n",
    "Disallow: /CHANGELOG.txt\n",
    "Disallow: /cron.php\n",
    "Disallow: /INSTALL.mysql.txt\n",
    "Disallow: /INSTALL.pgsql.txt\n",
    "Disallow: /INSTALL.sqlite.txt\n",
    "Disallow: /install.php\n",
    "Disallow: /INSTALL.txt\n",
    "Disallow: /LICENSE.txt\n",
    "Disallow: /MAINTAINERS.txt\n",
    "Disallow: /update.php\n",
    "Disallow: /UPGRADE.txt\n",
    "Disallow: /xmlrpc.php\n",
    "Paths (clean URLs)\n",
    "Disallow: /admin/\n",
    "Disallow: /comment/reply/\n",
    "Disallow: /filter/tips/\n",
    "Disallow: /node/add/\n",
    "Disallow: /search/\n",
    "Disallow: /user/register/\n",
    "Disallow: /user/password/\n",
    "Disallow: /user/login/\n",
    "Disallow: /user/logout/\n",
    "Paths (no clean URLs)\n",
    "Disallow: /?q=admin/\n",
    "Disallow: /?q=comment/reply/\n",
    "Disallow: /?q=filter/tips/\n",
    "Disallow: /?q=node/add/\n",
    "Disallow: /?q=search/\n",
    "Disallow: /?q=user/password/\n",
    "Disallow: /?q=user/register/\n",
    "Disallow: /?q=user/login/\n",
    "Disallow: /?q=user/logout/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/robots.txt\n",
    "\n",
    "Este sitio comparte en su robots un canal de contacto para personas interesadas en hacer crawling, y además de esto también comparte la URL para el API para el mismo fin, las directivas del crawling son las mismas para todos los agentes, sin embargo agrega una condición para el bot baidu, en la cual este tiene que usar un delay de 1 segundo, para el resto de agentes hay restricciones de rutas especificas que parecen corresponder a funcionalidades internas o para evitar cargas innecesarias, la unica ruta con allow espécifico es /*?tab=achievements&achievement=*, por ultimo, también restringe el acceso a account-login y a explodingstuff\n",
    "\n",
    "\n",
    "If you would like to crawl GitHub contact us via https://support.github.com?tags=dotcom-robots\n",
    "\n",
    "We also provide an extensive API: https://docs.github.com\n",
    "\n",
    "User-agent: baidu\n",
    "\n",
    "crawl-delay: 1\n",
    "\n",
    "\n",
    "User-agent: *\n",
    "\n",
    "Disallow: /*/*/pulse\n",
    "Disallow: /*/*/projects\n",
    "Disallow: /*/*/forks\n",
    "Disallow: /*/*/issues/new\n",
    "Disallow: /*/*/issues/search\n",
    "Disallow: /*/*/commits/\n",
    "Disallow: /*/*/branches\n",
    "Disallow: /*/*/contributors\n",
    "Disallow: /*/*/tags\n",
    "Disallow: /*/*/stargazers\n",
    "Disallow: /*/*/watchers\n",
    "Disallow: /*/*/network\n",
    "Disallow: /*/*/graphs\n",
    "Disallow: /*/*/compare\n",
    "\n",
    "Disallow: /*/tree/\n",
    "Disallow: /gist/\n",
    "Disallow: /*/download\n",
    "Disallow: /*/revisions\n",
    "Disallow: /*/commits/*?author\n",
    "Disallow: /*/commits/*?path\n",
    "Disallow: /*/comments\n",
    "Disallow: /*/archive/\n",
    "Disallow: /*/blame/\n",
    "Disallow: /*/raw/\n",
    "Disallow: /*/cache/\n",
    "Disallow: /.git/\n",
    "Disallow: */.git/\n",
    "Disallow: /*.git$\n",
    "Disallow: /search/advanced\n",
    "Disallow: /search$\n",
    "Disallow: /*q=\n",
    "Disallow: /*.atom$\n",
    "\n",
    "Disallow: /ekansa/Open-Context-Data\n",
    "Disallow: /ekansa/opencontext-*\n",
    "Disallow: */tarball/\n",
    "Disallow: */zipball/\n",
    "\n",
    "Disallow: /*source=*\n",
    "Disallow: /*ref_cta=*\n",
    "Disallow: /*plan=*\n",
    "Disallow: /*return_to=*\n",
    "Disallow: /*ref_loc=*\n",
    "Disallow: /*setup_organization=*\n",
    "Disallow: /*source_repo=*\n",
    "Disallow: /*ref_page=*\n",
    "Disallow: /*source=*\n",
    "Disallow: /*referrer=*\n",
    "Disallow: /*report=*\n",
    "Disallow: /*author=*\n",
    "Disallow: /*since=*\n",
    "Disallow: /*until=*\n",
    "Disallow: /*commits?author=*\n",
    "Disallow: /*report-abuse?report=*\n",
    "Disallow: /*tab=*\n",
    "Allow: /*?tab=achievements&achievement=*\n",
    "\n",
    "Disallow: /account-login\n",
    "Disallow: /Explodingstuff/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Práctica 2\n",
    "\n",
    "Con los sitios anteriores (y algún otro que tenga interés en revisar) y usando la biblioteca\n",
    "urllib.robotparser1 realizar lo siguiente:\n",
    "\n",
    "Abrir el archivo y establecer el parser, enseguida, con el método .can_fetch() determinar si su User Agent puede examinar\n",
    "el archivo, y luego, iterar sobre las diferentes entradas y reglas del archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping https://www.unam.mx/robots.txt due to error: HTTP Error 403: Forbidden\n",
      "Rules for https://www.cicese.mx/robots.txt:\n",
      "  User Agents: SemrushBot\n",
      "  Path: /\n",
      "  Allowed: No\n",
      "  User Agents: SemrushBot-SA\n",
      "  Path: /\n",
      "  Allowed: No\n",
      "  User Agents: SemrushBot-BM\n",
      "  Path: /\n",
      "  Allowed: No\n",
      "  User Agents: SemrushBot-CT\n",
      "  Path: /\n",
      "  Allowed: No\n",
      "  User Agents: SemrushBot-SWA\n",
      "  Path: /\n",
      "  Allowed: No\n",
      "  User Agents: SemrushBot-SI\n",
      "  Path: /\n",
      "  Allowed: No\n",
      "  User Agents: SemrushBot-BA\n",
      "  Path: /\n",
      "  Allowed: No\n",
      "  User Agents: BLEXBot\n",
      "  Path: /\n",
      "  Allowed: No\n",
      "\n",
      "\n",
      "Rules for https://www.uabc.mx/robots.txt:\n",
      "  User Agents: *\n",
      "  Path: /wp-admin/\n",
      "  Allowed: No\n",
      "  User Agents: *\n",
      "  Path: /wp-admin/admin-ajax.php\n",
      "  Allowed: Yes\n",
      "  User Agents: *\n",
      "  Path: /wp-content/uploads/wpforms/\n",
      "  Allowed: No\n",
      "\n",
      "\n",
      "Rules for https://www.github.com/robots.txt:\n",
      "  No rules found. Here's the original robots.txt content:\n",
      "# If you would like to crawl GitHub contact us via https://support.github.com?tags=dotcom-robots\n",
      "# We also provide an extensive API: https://docs.github.com\n",
      "User-agent: baidu\n",
      "crawl-delay: 1\n",
      "\n",
      "\n",
      "User-agent: *\n",
      "\n",
      "Disallow: /*/*/pulse\n",
      "Disallow: /*/*/projects\n",
      "Disallow: /*/*/forks\n",
      "Disallow: /*/*/issues/new\n",
      "Disallow: /*/*/issues/search\n",
      "Disallow: /*/*/commits/\n",
      "Disallow: /*/*/branches\n",
      "Disallow: /*/*/contributors\n",
      "Disallow: /*/*/tags\n",
      "Disallow: /*/*/stargazers\n",
      "Disallow: /*/*/watchers\n",
      "Disallow: /*/*/network\n",
      "Disallow: /*/*/graphs\n",
      "Disallow: /*/*/compare\n",
      "\n",
      "Disallow: /*/tree/\n",
      "Disallow: /gist/\n",
      "Disallow: /*/download\n",
      "Disallow: /*/revisions\n",
      "Disallow: /*/commits/*?author\n",
      "Disallow: /*/commits/*?path\n",
      "Disallow: /*/comments\n",
      "Disallow: /*/archive/\n",
      "Disallow: /*/blame/\n",
      "Disallow: /*/raw/\n",
      "Disallow: /*/cache/\n",
      "Disallow: /.git/\n",
      "Disallow: */.git/\n",
      "Disallow: /*.git$\n",
      "Disallow: /search/advanced\n",
      "Disallow: /search$\n",
      "Disallow: /*q=\n",
      "Disallow: /*.atom$\n",
      "\n",
      "Disallow: /ekansa/Open-Context-Data\n",
      "Disallow: /ekansa/opencontext-*\n",
      "Disallow: */tarball/\n",
      "Disallow: */zipball/\n",
      "\n",
      "Disallow: /*source=*\n",
      "Disallow: /*ref_cta=*\n",
      "Disallow: /*plan=*\n",
      "Disallow: /*return_to=*\n",
      "Disallow: /*ref_loc=*\n",
      "Disallow: /*setup_organization=*\n",
      "Disallow: /*source_repo=*\n",
      "Disallow: /*ref_page=*\n",
      "Disallow: /*source=*\n",
      "Disallow: /*referrer=*\n",
      "Disallow: /*report=*\n",
      "Disallow: /*author=*\n",
      "Disallow: /*since=*\n",
      "Disallow: /*until=*\n",
      "Disallow: /*commits?author=*\n",
      "Disallow: /*report-abuse?report=*\n",
      "Disallow: /*tab=*\n",
      "Allow: /*?tab=achievements&achievement=*\n",
      "\n",
      "Disallow: /account-login\n",
      "Disallow: /Explodingstuff/\n",
      "\n",
      "\n",
      "Rules for https://www.halopedia.org/robots.txt:\n",
      "  User Agents: Mediapartners-Google\n",
      "  Path: \n",
      "  Allowed: Yes\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib.robotparser\n",
    "import ssl\n",
    "import urllib.request\n",
    "\n",
    "# Ignorar certificado SSL\n",
    "ssl_context = ssl._create_unverified_context()\n",
    "\n",
    "websites = [\n",
    "    \"https://www.cicese.mx/robots.txt\",\n",
    "    \"https://www.uabc.mx/robots.txt\",\n",
    "    \"https://www.unam.mx/robots.txt\",\n",
    "    \"https://www.github.com/robots.txt\",\n",
    "    \"https://www.halopedia.org/robots.txt\"\n",
    "]\n",
    "\n",
    "# Función para revisar con can_fetch\n",
    "def can_fetch(user_agent, url):\n",
    "    rp = urllib.robotparser.RobotFileParser()\n",
    "    rp.set_url(url)\n",
    "    rp.read()\n",
    "    return rp.can_fetch(user_agent, url)\n",
    "\n",
    "# Función para iterar las reglas\n",
    "def iterate_rules(robots_content):\n",
    "    rfp = urllib.robotparser.RobotFileParser()\n",
    "    rfp.parse(robots_content.splitlines())\n",
    "    entries = [rfp.default_entry, *rfp.entries] if rfp.default_entry else rfp.entries\n",
    "\n",
    "    for entry in entries:\n",
    "        for ruleline in entry.rulelines:\n",
    "            yield (entry.useragents, ruleline.path, ruleline.allowance)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for site in websites:\n",
    "    try:\n",
    "        with urllib.request.urlopen(site, context=ssl_context) as response:\n",
    "            robots_content = response.read().decode('utf-8')\n",
    "            rules = list(iterate_rules(robots_content))\n",
    "            results[site] = (robots_content, rules)  # Save both content and rules\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {site} due to error: {e}\")\n",
    "        continue\n",
    "\n",
    "# Desplegar resultados\n",
    "for site, (robots_content, rules) in results.items():\n",
    "    print(f\"Rules for {site}:\")\n",
    "    if not rules:\n",
    "        print(\"  No rules found. Here's the original robots.txt content:\")\n",
    "        print(robots_content.strip() or \"  (robots.txt is empty)\")\n",
    "    else:\n",
    "        for rule in rules:\n",
    "            useragents, path, allowance = rule\n",
    "            print(f\"  User Agents: {', '.join(useragents) if useragents else '*'}\")\n",
    "            print(f\"  Path: {path}\")\n",
    "            print(f\"  Allowed: {'Yes' if allowance else 'No'}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver en la consola, tres de los sitios visitados tienen reglas especificas para el agente, un sitio no tienen reglas y un sitio da error 403, lo cual indica que el servidor está bloqueando mi solicitud, después de intentar con distintos agentes llegué a lo mismo con este sitio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping https://www.unam.mx/robots.txt due to error: HTTP Error 403: Forbidden\n",
      "Rules for https://www.cicese.mx/robots.txt:\n",
      "  User Agents: SemrushBot\n",
      "  Path: /\n",
      "  Allowed: No\n",
      "  User Agents: SemrushBot-SA\n",
      "  Path: /\n",
      "  Allowed: No\n",
      "  User Agents: SemrushBot-BM\n",
      "  Path: /\n",
      "  Allowed: No\n",
      "  User Agents: SemrushBot-CT\n",
      "  Path: /\n",
      "  Allowed: No\n",
      "  User Agents: SemrushBot-SWA\n",
      "  Path: /\n",
      "  Allowed: No\n",
      "  User Agents: SemrushBot-SI\n",
      "  Path: /\n",
      "  Allowed: No\n",
      "  User Agents: SemrushBot-BA\n",
      "  Path: /\n",
      "  Allowed: No\n",
      "  User Agents: BLEXBot\n",
      "  Path: /\n",
      "  Allowed: No\n",
      "\n",
      "\n",
      "Rules for https://www.uabc.mx/robots.txt:\n",
      "  User Agents: *\n",
      "  Path: /wp-admin/\n",
      "  Allowed: No\n",
      "  User Agents: *\n",
      "  Path: /wp-admin/admin-ajax.php\n",
      "  Allowed: Yes\n",
      "  User Agents: *\n",
      "  Path: /wp-content/uploads/wpforms/\n",
      "  Allowed: No\n",
      "\n",
      "\n",
      "Rules for https://www.github.com/robots.txt:\n",
      "  No rules found. Here's the original robots.txt content:\n",
      "# If you would like to crawl GitHub contact us via https://support.github.com?tags=dotcom-robots\n",
      "# We also provide an extensive API: https://docs.github.com\n",
      "User-agent: baidu\n",
      "crawl-delay: 1\n",
      "\n",
      "\n",
      "User-agent: *\n",
      "\n",
      "Disallow: /*/*/pulse\n",
      "Disallow: /*/*/projects\n",
      "Disallow: /*/*/forks\n",
      "Disallow: /*/*/issues/new\n",
      "Disallow: /*/*/issues/search\n",
      "Disallow: /*/*/commits/\n",
      "Disallow: /*/*/branches\n",
      "Disallow: /*/*/contributors\n",
      "Disallow: /*/*/tags\n",
      "Disallow: /*/*/stargazers\n",
      "Disallow: /*/*/watchers\n",
      "Disallow: /*/*/network\n",
      "Disallow: /*/*/graphs\n",
      "Disallow: /*/*/compare\n",
      "\n",
      "Disallow: /*/tree/\n",
      "Disallow: /gist/\n",
      "Disallow: /*/download\n",
      "Disallow: /*/revisions\n",
      "Disallow: /*/commits/*?author\n",
      "Disallow: /*/commits/*?path\n",
      "Disallow: /*/comments\n",
      "Disallow: /*/archive/\n",
      "Disallow: /*/blame/\n",
      "Disallow: /*/raw/\n",
      "Disallow: /*/cache/\n",
      "Disallow: /.git/\n",
      "Disallow: */.git/\n",
      "Disallow: /*.git$\n",
      "Disallow: /search/advanced\n",
      "Disallow: /search$\n",
      "Disallow: /*q=\n",
      "Disallow: /*.atom$\n",
      "\n",
      "Disallow: /ekansa/Open-Context-Data\n",
      "Disallow: /ekansa/opencontext-*\n",
      "Disallow: */tarball/\n",
      "Disallow: */zipball/\n",
      "\n",
      "Disallow: /*source=*\n",
      "Disallow: /*ref_cta=*\n",
      "Disallow: /*plan=*\n",
      "Disallow: /*return_to=*\n",
      "Disallow: /*ref_loc=*\n",
      "Disallow: /*setup_organization=*\n",
      "Disallow: /*source_repo=*\n",
      "Disallow: /*ref_page=*\n",
      "Disallow: /*source=*\n",
      "Disallow: /*referrer=*\n",
      "Disallow: /*report=*\n",
      "Disallow: /*author=*\n",
      "Disallow: /*since=*\n",
      "Disallow: /*until=*\n",
      "Disallow: /*commits?author=*\n",
      "Disallow: /*report-abuse?report=*\n",
      "Disallow: /*tab=*\n",
      "Allow: /*?tab=achievements&achievement=*\n",
      "\n",
      "Disallow: /account-login\n",
      "Disallow: /Explodingstuff/\n",
      "\n",
      "\n",
      "Rules for https://www.halopedia.org/robots.txt:\n",
      "  User Agents: Mediapartners-Google\n",
      "  Path: \n",
      "  Allowed: Yes\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import urllib.robotparser\n",
    "import ssl\n",
    "import urllib.request\n",
    "\n",
    "ssl_context = ssl._create_unverified_context()\n",
    "\n",
    "websites = [\n",
    "    \"https://www.cicese.mx/robots.txt\",\n",
    "    \"https://www.uabc.mx/robots.txt\",\n",
    "    \"https://www.unam.mx/robots.txt\",\n",
    "    \"https://www.github.com/robots.txt\",\n",
    "    \"https://www.halopedia.org/robots.txt\"\n",
    "]\n",
    "\n",
    "# Función para crear patrones regex a partir de las rutas de robots.txt\n",
    "# Esto permite manejar wildcards (*) y el símbolo de fin de línea ($)\n",
    "def get_robots_pattern(path):\n",
    "    ending = '.*'  # Por defecto, permitimos cualquier cosa después de la ruta\n",
    "    if path.endswith('$'):\n",
    "        path = path[:-1]  # Eliminar $ \n",
    "        ending = '$'\n",
    "    parts = path.split('*')  # Dividir la ruta donde haya '*'\n",
    "    parts = map(urllib.parse.quote, map(re.escape, parts))  \n",
    "    return '.*'.join(parts) + ending  # Reunir los fragmentos en un patrón regex\n",
    "\n",
    "# Parche para la clase RuleLine de urllib.robotparser qque permite wildcards\n",
    "def _rule_line_init__(self, path, allowance):\n",
    "    if path == '' and not allowance:\n",
    "        allowance = True\n",
    "    path = urllib.parse.unquote(urllib.parse.unquote(path))  # Decodificar URL\n",
    "    self.pattern = re.compile(get_robots_pattern(path)) \n",
    "    self.path = path  \n",
    "    self.allowance = allowance  # Guardar si se permite (True) o no (False)\n",
    "\n",
    "# Verificar si la regla aplica a una URL específica\n",
    "# Devuelve True si la URL coincide con el patrón de la regla\n",
    "def _rule_line_applies_to(self, filename):\n",
    "    return True if self.pattern.match(filename) else False\n",
    "\n",
    "# Aplicar el parche a la clase RuleLine\n",
    "urllib.robotparser.RuleLine.__init__ = _rule_line_init__\n",
    "urllib.robotparser.RuleLine.applies_to = _rule_line_applies_to\n",
    "\n",
    "# Función para iterar sobre las reglas\n",
    "def iterate_rules(robots_content):\n",
    "    rfp = urllib.robotparser.RobotFileParser()\n",
    "    rfp.parse(robots_content.splitlines())  # Analizar el contenido línea por línea\n",
    "    entries = [rfp.default_entry, *rfp.entries] if rfp.default_entry else rfp.entries\n",
    "\n",
    "    for entry in entries:\n",
    "        for ruleline in entry.rulelines:\n",
    "            yield (entry.useragents, ruleline.path, ruleline.allowance)  # Devolver User-Agent, ruta y permiso\n",
    "\n",
    "results = {}\n",
    "\n",
    "for site in websites:\n",
    "    try:\n",
    "        with urllib.request.urlopen(site, context=ssl_context) as response:\n",
    "            robots_content = response.read().decode('utf-8')  # Leer y decodificar el contenido\n",
    "            rules = list(iterate_rules(robots_content))  # Obtener las reglas\n",
    "            results[site] = (robots_content, rules)  # Guardar tanto el contenido como las reglas\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {site} due to error: {e}\")  # Saltar el sitio en caso de error\n",
    "        continue\n",
    "\n",
    "# Mostrar los resultados\n",
    "for site, (robots_content, rules) in results.items():\n",
    "    print(f\"Rules for {site}:\")\n",
    "    if not rules:\n",
    "        print(\"  No rules found. Here's the original robots.txt content:\")\n",
    "        print(robots_content.strip() or \"  (robots.txt is empty)\")  # Imprimir el contenido si no hay reglas\n",
    "    else:\n",
    "        for rule in rules:\n",
    "            useragents, path, allowance = rule\n",
    "            print(f\"  User Agents: {', '.join(useragents) if useragents else '*'}\")  # Imprimir User-Agent\n",
    "            print(f\"  Path: {path}\")  # Imprimir la ruta\n",
    "            print(f\"  Allowed: {'Yes' if allowance else 'No'}\")  # Indicar si está permitido o no\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta parte del código decidí agregar más detalles en los comentarios ya que el procesado lo saqué del ejemplo y no tengo mucho que agregar de mi parte más que el entendimiento del proceso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Práctica 3\n",
    "\n",
    "Algunos sitios también ofrecen un mapa del sitio, por lo que de ser así, conviene\n",
    "analizarlo. Entonces, con los sitios que revisó anteriormente, verifique mediante su\n",
    "browser si existe el archivo sitemap.xml (o bien revise en robots.txt si se indica\n",
    "alguno) y comente sobre lo que encuentre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install advertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sitemaps found:\n",
      "    https://www.halopedia.org/sitemap/sitemap-index-wiki.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 16:35:35,670 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_12-0.xml.gz\n",
      "2025-02-03 16:35:35,672 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_100-0.xml.gz\n",
      "2025-02-03 16:35:35,673 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_104-0.xml.gz\n",
      "2025-02-03 16:35:35,700 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_4-0.xml.gz\n",
      "2025-02-03 16:35:35,829 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_6-1.xml.gz\n",
      "2025-02-03 16:35:35,867 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_14-0.xml.gz\n",
      "2025-02-03 16:35:35,927 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_102-0.xml.gz\n",
      "2025-02-03 16:35:36,025 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_0-0.xml.gz\n",
      "2025-02-03 16:35:36,299 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_6-0.xml.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Could not process sitemap: \"['changefreq'] not in index\"\n",
      "\n",
      "\n",
      "  Sitemaps found:\n",
      "    https://www.halopedia.org/sitemap/sitemap-index-wiki.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 16:35:58,504 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_4-0.xml.gz\n",
      "2025-02-03 16:35:58,507 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_104-0.xml.gz\n",
      "2025-02-03 16:35:58,517 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_12-0.xml.gz\n",
      "2025-02-03 16:35:58,518 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_100-0.xml.gz\n",
      "2025-02-03 16:35:58,667 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_6-1.xml.gz\n",
      "2025-02-03 16:35:58,709 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_14-0.xml.gz\n",
      "2025-02-03 16:35:58,767 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_102-0.xml.gz\n",
      "2025-02-03 16:35:58,858 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_0-0.xml.gz\n",
      "2025-02-03 16:35:59,201 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_6-0.xml.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Could not process sitemap: \"['changefreq'] not in index\"\n",
      "\n",
      "\n",
      "Skipping https://www.unam.mx/robots.txt due to error: HTTP Error 403: Forbidden\n",
      "  Sitemaps found:\n",
      "    https://www.halopedia.org/sitemap/sitemap-index-wiki.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 16:36:00,572 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_12-0.xml.gz\n",
      "2025-02-03 16:36:00,575 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_104-0.xml.gz\n",
      "2025-02-03 16:36:00,665 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_100-0.xml.gz\n",
      "2025-02-03 16:36:00,667 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_4-0.xml.gz\n",
      "2025-02-03 16:36:00,746 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_14-0.xml.gz\n",
      "2025-02-03 16:36:00,791 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_6-1.xml.gz\n",
      "2025-02-03 16:36:00,957 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_0-0.xml.gz\n",
      "2025-02-03 16:36:00,961 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_102-0.xml.gz\n",
      "2025-02-03 16:36:01,372 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_6-0.xml.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Could not process sitemap: \"['changefreq'] not in index\"\n",
      "\n",
      "\n",
      "  Sitemaps found:\n",
      "    https://www.halopedia.org/sitemap/sitemap-index-wiki.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 16:36:02,545 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_104-0.xml.gz\n",
      "2025-02-03 16:36:02,548 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_12-0.xml.gz\n",
      "2025-02-03 16:36:02,549 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_100-0.xml.gz\n",
      "2025-02-03 16:36:02,550 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_4-0.xml.gz\n",
      "2025-02-03 16:36:02,705 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_14-0.xml.gz\n",
      "2025-02-03 16:36:02,738 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_6-1.xml.gz\n",
      "2025-02-03 16:36:02,797 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_102-0.xml.gz\n",
      "2025-02-03 16:36:02,903 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_0-0.xml.gz\n",
      "2025-02-03 16:36:03,438 | INFO | sitemaps.py:623 | sitemap_to_df | Getting https://www.halopedia.org/sitemap/sitemap-wiki-NS_6-0.xml.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Could not process sitemap: \"['changefreq'] not in index\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import advertools as adv  # Para analizar los sitemaps\n",
    "\n",
    "def extract_sitemap(robots_content):\n",
    "    sitemaps = re.findall(r'Sitemap:\\s*(\\S+)', robots_content)\n",
    "    return sitemaps\n",
    "\n",
    "for site in websites:\n",
    "    try:\n",
    "        with urllib.request.urlopen(site, context=ssl_context) as response:\n",
    "            sitemaps = extract_sitemap(robots_content)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {site} due to error: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Procesar y mostrar el sitemap si existe\n",
    "    if sitemaps:\n",
    "        print(\"  Sitemaps found:\")\n",
    "        for sitemap_url in sitemaps:\n",
    "            print(f\"    {sitemap_url}\")\n",
    "            try:\n",
    "                sitemap_df = adv.sitemap_to_df(sitemap_url)\n",
    "                print(sitemap_df[['loc', 'lastmod', 'changefreq', 'priority']].head())\n",
    "            except Exception as e:\n",
    "                print(f\"    Could not process sitemap: {e}\")\n",
    "    else:\n",
    "        print(\"  No sitemap found.\")\n",
    "\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el procesado del sitemap se usó advertools, esta herramienta cuenta con una buena documentación al respecto de robots y sitemaps en https://advertools.readthedocs.io/en/master/advertools.sitemaps.html, a pesar de ser solo el final de la práctica la documentación es interesante y tiene buenos ejemplos. Respecto a la página de unam, intenté con otros agentes, sin embargo me seguía bloqueando y las alternativas que se me ocurrieron para poder acceder se desviaban mucho de la práctica, por lo que para el reporte lo dejé así "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
